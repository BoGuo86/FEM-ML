{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Classification - MNIST with CNN model usign TensorFlow\n",
    "Last Updated : 04/05/2019, by Hyungmin Jun (hyungminjun@outlook.com)\n",
    "\n",
    "=============================================================================\n",
    "\n",
    "Copyright 2019 Hyungmin Jun. All rights reserved.\n",
    "\n",
    "License - GPL version 3\n",
    "This program is free software: you can redistribute it and/or modify it under\n",
    "the terms of the GNU General Public License as published by the Free Software\n",
    "Foundation, either version 3 of the License, or any later version. This\n",
    "program is distributed in the hope that it will be useful, but WITHOUT ANY\n",
    "WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR\n",
    "A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n",
    "You should have received a copy of the GNU General Public License along with\n",
    "this program. If not, see <http://www.gnu.org/licenses/>.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from datetime import datetime\n",
    "\n",
    "mnist = input_data.read_data_sets(\"../data/MNIST/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train image shape =  (55000, 784)\n",
      "train label shape =  (55000, 10)\n",
      "test image shape =  (10000, 784)\n",
      "test label shape =  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\ntrain image shape = \", np.shape(mnist.train.images))\n",
    "print(\"train label shape = \",   np.shape(mnist.train.labels))\n",
    "print(\"test image shape = \",    np.shape(mnist.test.images))\n",
    "print(\"test label shape = \",    np.shape(mnist.test.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter\n",
    "learning_rate = 0.01\n",
    "epochs        = 30\n",
    "batch_size    = 100\n",
    "\n",
    "# Set the placeholder for input and output\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "T = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Input layer and reshape for convolution\n",
    "# ==================================================\n",
    "A1 = X_img = tf.reshape(X, [-1, 28, 28, 1])   # Image size 28 by 28 by 1 (gray image)\n",
    "\n",
    "# ==================================================\n",
    "# 1st convolution layer, using 3 by 3 with 32 filters\n",
    "# ==================================================\n",
    "F2 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "B2 = tf.Variable(tf.constant(0.1, shape=[32]))\n",
    "\n",
    "# 28 by 28 by 1 to 28 by 28 by 32\n",
    "C2 = tf.nn.conv2d(A1, F2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# Relu activation function\n",
    "Z2 = tf.nn.relu(C2 + B2)\n",
    "\n",
    "# Max pooling; 28 by 28 by 32 to 14 by 14 by 32\n",
    "A2 = P2 = tf.nn.max_pool(Z2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# ==================================================\n",
    "# 2nd convolution layer, using 3 by 3 with 64 filters\n",
    "# ==================================================\n",
    "F3 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "B3 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "\n",
    "# 14 by 14 by 32 to 14 by 14 by 64\n",
    "C3 = tf.nn.conv2d(A2, F3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# Relu activation function\n",
    "Z3 = tf.nn.relu(C3 + B3)\n",
    "\n",
    "# Max pooling; 14 by 14 by 64 to 7 by 7 by 64\n",
    "A3 = P3 = tf.nn.max_pool(Z3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# ==================================================\n",
    "# 3rd convolution layer, using 3 by 3 with 128 filters\n",
    "# ==================================================\n",
    "F4 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
    "B4 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "\n",
    "# 7 by 7 by 64 to 7 by 7 by 128\n",
    "C4 = tf.nn.conv2d(A3, F4, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# Relu activation function\n",
    "Z4 = tf.nn.relu(C4 + B4)\n",
    "\n",
    "# Max pooling; 7 by 7 by 128 to 4 by 4 by 128\n",
    "A4 = P4 = tf.nn.max_pool(Z4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# ==================================================\n",
    "# Flatten layer; 4 by 4 with 128 activation maps\n",
    "# ==================================================\n",
    "A4_flat = P4_flat = tf.reshape(A4, [-1, 128*4*4])\n",
    "\n",
    "# ==================================================\n",
    "# Output layer\n",
    "# ==================================================\n",
    "W5 = tf.Variable(tf.random_normal([128*4*4, 10], stddev=0.01))\n",
    "B5 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# Linear regression\n",
    "Z5 = logits = tf.matmul(A4_flat, W5) + B5\n",
    "\n",
    "Y = A5 = tf.nn.softmax(Z5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z5, labels=T))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# Compare batch_size X by argmax\n",
    "predicted_val = tf.equal(tf.argmax(A5, 1), tf.argmax(T, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(predicted_val, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs =  0 , step =  0 , loss_val =  2.8012495\n",
      "epochs =  0 , step =  100 , loss_val =  0.17730127\n",
      "epochs =  0 , step =  200 , loss_val =  0.116296805\n",
      "epochs =  0 , step =  300 , loss_val =  0.07609382\n",
      "epochs =  0 , step =  400 , loss_val =  0.070986964\n",
      "epochs =  0 , step =  500 , loss_val =  0.26834992\n",
      "epochs =  1 , step =  0 , loss_val =  0.118091494\n",
      "epochs =  1 , step =  100 , loss_val =  0.032817334\n",
      "epochs =  1 , step =  200 , loss_val =  0.080333225\n",
      "epochs =  1 , step =  300 , loss_val =  0.033648722\n",
      "epochs =  1 , step =  400 , loss_val =  0.062489204\n",
      "epochs =  1 , step =  500 , loss_val =  0.07159407\n",
      "epochs =  2 , step =  0 , loss_val =  0.010054498\n",
      "epochs =  2 , step =  100 , loss_val =  0.012050558\n",
      "epochs =  2 , step =  200 , loss_val =  0.011817139\n",
      "epochs =  2 , step =  300 , loss_val =  0.008080794\n",
      "epochs =  2 , step =  400 , loss_val =  0.2861688\n",
      "epochs =  2 , step =  500 , loss_val =  0.007543987\n",
      "epochs =  3 , step =  0 , loss_val =  0.06371533\n",
      "epochs =  3 , step =  100 , loss_val =  0.02802482\n",
      "epochs =  3 , step =  200 , loss_val =  0.047438096\n",
      "epochs =  3 , step =  300 , loss_val =  0.045627844\n",
      "epochs =  3 , step =  400 , loss_val =  0.0633512\n",
      "epochs =  3 , step =  500 , loss_val =  0.026882323\n",
      "epochs =  4 , step =  0 , loss_val =  0.088506214\n",
      "epochs =  4 , step =  100 , loss_val =  0.0035192275\n",
      "epochs =  4 , step =  200 , loss_val =  0.13639939\n",
      "epochs =  4 , step =  300 , loss_val =  0.12638876\n",
      "epochs =  4 , step =  400 , loss_val =  0.016458211\n",
      "epochs =  4 , step =  500 , loss_val =  0.09484815\n",
      "epochs =  5 , step =  0 , loss_val =  0.008062806\n",
      "epochs =  5 , step =  100 , loss_val =  0.03550612\n",
      "epochs =  5 , step =  200 , loss_val =  0.05806261\n",
      "epochs =  5 , step =  300 , loss_val =  0.03955149\n",
      "epochs =  5 , step =  400 , loss_val =  0.09951015\n",
      "epochs =  5 , step =  500 , loss_val =  0.0876029\n",
      "epochs =  6 , step =  0 , loss_val =  0.019938458\n",
      "epochs =  6 , step =  100 , loss_val =  0.021298049\n",
      "epochs =  6 , step =  200 , loss_val =  0.01702853\n",
      "epochs =  6 , step =  300 , loss_val =  0.010016752\n",
      "epochs =  6 , step =  400 , loss_val =  0.014568427\n",
      "epochs =  6 , step =  500 , loss_val =  0.0075288196\n",
      "epochs =  7 , step =  0 , loss_val =  0.02493962\n",
      "epochs =  7 , step =  100 , loss_val =  0.04934837\n",
      "epochs =  7 , step =  200 , loss_val =  0.025026822\n",
      "epochs =  7 , step =  300 , loss_val =  0.0028323615\n",
      "epochs =  7 , step =  400 , loss_val =  0.07775415\n",
      "epochs =  7 , step =  500 , loss_val =  0.052709136\n",
      "epochs =  8 , step =  0 , loss_val =  0.11558464\n",
      "epochs =  8 , step =  100 , loss_val =  0.072494976\n",
      "epochs =  8 , step =  200 , loss_val =  0.09213764\n",
      "epochs =  8 , step =  300 , loss_val =  0.027038248\n",
      "epochs =  8 , step =  400 , loss_val =  0.03635526\n",
      "epochs =  8 , step =  500 , loss_val =  0.20641395\n",
      "epochs =  9 , step =  0 , loss_val =  0.082673065\n",
      "epochs =  9 , step =  100 , loss_val =  0.09293579\n",
      "epochs =  9 , step =  200 , loss_val =  0.04843567\n",
      "epochs =  9 , step =  300 , loss_val =  0.015010846\n",
      "epochs =  9 , step =  400 , loss_val =  0.095002525\n",
      "epochs =  9 , step =  500 , loss_val =  0.122991055\n",
      "epochs =  10 , step =  0 , loss_val =  0.12859169\n",
      "epochs =  10 , step =  100 , loss_val =  0.007841002\n",
      "epochs =  10 , step =  200 , loss_val =  0.0058589177\n",
      "epochs =  10 , step =  300 , loss_val =  0.036554053\n",
      "epochs =  10 , step =  400 , loss_val =  0.028421769\n",
      "epochs =  10 , step =  500 , loss_val =  0.026159756\n",
      "epochs =  11 , step =  0 , loss_val =  0.004015389\n",
      "epochs =  11 , step =  100 , loss_val =  0.00047918918\n",
      "epochs =  11 , step =  200 , loss_val =  0.04068553\n",
      "epochs =  11 , step =  300 , loss_val =  0.060650572\n",
      "epochs =  11 , step =  400 , loss_val =  0.045218095\n",
      "epochs =  11 , step =  500 , loss_val =  0.0026943323\n",
      "epochs =  12 , step =  0 , loss_val =  0.17848998\n",
      "epochs =  12 , step =  100 , loss_val =  0.00023893907\n",
      "epochs =  12 , step =  200 , loss_val =  0.031569026\n",
      "epochs =  12 , step =  300 , loss_val =  2.7942182e-05\n",
      "epochs =  12 , step =  400 , loss_val =  0.021080786\n",
      "epochs =  12 , step =  500 , loss_val =  0.040674925\n",
      "epochs =  13 , step =  0 , loss_val =  0.05439099\n",
      "epochs =  13 , step =  100 , loss_val =  0.012443737\n",
      "epochs =  13 , step =  200 , loss_val =  0.00016265849\n",
      "epochs =  13 , step =  300 , loss_val =  0.02211895\n",
      "epochs =  13 , step =  400 , loss_val =  0.15013905\n",
      "epochs =  13 , step =  500 , loss_val =  0.027960166\n",
      "epochs =  14 , step =  0 , loss_val =  0.014431247\n",
      "epochs =  14 , step =  100 , loss_val =  0.020876177\n",
      "epochs =  14 , step =  200 , loss_val =  0.0017830478\n",
      "epochs =  14 , step =  300 , loss_val =  0.019914009\n",
      "epochs =  14 , step =  400 , loss_val =  0.0006568484\n",
      "epochs =  14 , step =  500 , loss_val =  0.00621821\n",
      "epochs =  15 , step =  0 , loss_val =  0.20109014\n",
      "epochs =  15 , step =  100 , loss_val =  0.033302505\n",
      "epochs =  15 , step =  200 , loss_val =  0.0012294911\n",
      "epochs =  15 , step =  300 , loss_val =  0.006614672\n",
      "epochs =  15 , step =  400 , loss_val =  0.062234145\n",
      "epochs =  15 , step =  500 , loss_val =  0.04366566\n",
      "epochs =  16 , step =  0 , loss_val =  0.000364008\n",
      "epochs =  16 , step =  100 , loss_val =  0.0016747476\n",
      "epochs =  16 , step =  200 , loss_val =  0.02019938\n",
      "epochs =  16 , step =  300 , loss_val =  0.0060923737\n",
      "epochs =  16 , step =  400 , loss_val =  0.061318778\n",
      "epochs =  16 , step =  500 , loss_val =  0.0035560103\n",
      "epochs =  17 , step =  0 , loss_val =  0.0021920025\n",
      "epochs =  17 , step =  100 , loss_val =  3.147192e-05\n",
      "epochs =  17 , step =  200 , loss_val =  0.20641989\n",
      "epochs =  17 , step =  300 , loss_val =  0.03946889\n",
      "epochs =  17 , step =  400 , loss_val =  0.0047263335\n",
      "epochs =  17 , step =  500 , loss_val =  0.035521638\n",
      "epochs =  18 , step =  0 , loss_val =  0.2280213\n",
      "epochs =  18 , step =  100 , loss_val =  0.001329893\n",
      "epochs =  18 , step =  200 , loss_val =  0.0019886866\n",
      "epochs =  18 , step =  300 , loss_val =  0.0012890229\n",
      "epochs =  18 , step =  400 , loss_val =  0.0004590815\n",
      "epochs =  18 , step =  500 , loss_val =  0.09129953\n",
      "epochs =  19 , step =  0 , loss_val =  2.6887341e-05\n",
      "epochs =  19 , step =  100 , loss_val =  0.06385941\n",
      "epochs =  19 , step =  200 , loss_val =  2.7033042e-05\n",
      "epochs =  19 , step =  300 , loss_val =  0.0011956576\n",
      "epochs =  19 , step =  400 , loss_val =  0.003390495\n",
      "epochs =  19 , step =  500 , loss_val =  0.07522947\n",
      "epochs =  20 , step =  0 , loss_val =  0.00044145313\n",
      "epochs =  20 , step =  100 , loss_val =  0.015491974\n",
      "epochs =  20 , step =  200 , loss_val =  0.012180051\n",
      "epochs =  20 , step =  300 , loss_val =  0.00018916372\n",
      "epochs =  20 , step =  400 , loss_val =  4.391325e-06\n",
      "epochs =  20 , step =  500 , loss_val =  0.013831076\n",
      "epochs =  21 , step =  0 , loss_val =  0.013323437\n",
      "epochs =  21 , step =  100 , loss_val =  1.6977761e-05\n",
      "epochs =  21 , step =  200 , loss_val =  0.00029142632\n",
      "epochs =  21 , step =  300 , loss_val =  0.056621622\n",
      "epochs =  21 , step =  400 , loss_val =  0.002798519\n",
      "epochs =  21 , step =  500 , loss_val =  0.22132643\n",
      "epochs =  22 , step =  0 , loss_val =  0.078482\n",
      "epochs =  22 , step =  100 , loss_val =  0.29184115\n",
      "epochs =  22 , step =  200 , loss_val =  0.00052224053\n",
      "epochs =  22 , step =  300 , loss_val =  0.05227075\n",
      "epochs =  22 , step =  400 , loss_val =  0.0014880768\n",
      "epochs =  22 , step =  500 , loss_val =  0.060708478\n",
      "epochs =  23 , step =  0 , loss_val =  0.07804397\n",
      "epochs =  23 , step =  100 , loss_val =  0.12683281\n",
      "epochs =  23 , step =  200 , loss_val =  0.037286066\n",
      "epochs =  23 , step =  300 , loss_val =  0.006490374\n",
      "epochs =  23 , step =  400 , loss_val =  5.907174e-06\n",
      "epochs =  23 , step =  500 , loss_val =  0.00040000264\n",
      "epochs =  24 , step =  0 , loss_val =  0.0020548839\n",
      "epochs =  24 , step =  100 , loss_val =  0.004503504\n",
      "epochs =  24 , step =  200 , loss_val =  0.051633455\n",
      "epochs =  24 , step =  300 , loss_val =  3.593456e-05\n",
      "epochs =  24 , step =  400 , loss_val =  0.032181155\n",
      "epochs =  24 , step =  500 , loss_val =  0.051795002\n",
      "epochs =  25 , step =  0 , loss_val =  1.7415491e-06\n",
      "epochs =  25 , step =  100 , loss_val =  0.0028160813\n",
      "epochs =  25 , step =  200 , loss_val =  0.18651722\n",
      "epochs =  25 , step =  300 , loss_val =  0.09948571\n",
      "epochs =  25 , step =  400 , loss_val =  0.081262015\n",
      "epochs =  25 , step =  500 , loss_val =  0.015277072\n",
      "epochs =  26 , step =  0 , loss_val =  0.08509812\n",
      "epochs =  26 , step =  100 , loss_val =  0.06689478\n",
      "epochs =  26 , step =  200 , loss_val =  2.3841857e-09\n",
      "epochs =  26 , step =  300 , loss_val =  0.00041201746\n",
      "epochs =  26 , step =  400 , loss_val =  0.05907467\n",
      "epochs =  26 , step =  500 , loss_val =  0.09788439\n",
      "epochs =  27 , step =  0 , loss_val =  0.038164355\n",
      "epochs =  27 , step =  100 , loss_val =  0.035808723\n",
      "epochs =  27 , step =  200 , loss_val =  0.06350934\n",
      "epochs =  27 , step =  300 , loss_val =  0.094770946\n",
      "epochs =  27 , step =  400 , loss_val =  0.00011019692\n",
      "epochs =  27 , step =  500 , loss_val =  0.061800376\n",
      "epochs =  28 , step =  0 , loss_val =  2.1099862e-07\n",
      "epochs =  28 , step =  100 , loss_val =  0.0010776457\n",
      "epochs =  28 , step =  200 , loss_val =  1.2278487e-07\n",
      "epochs =  28 , step =  300 , loss_val =  1.3708988e-07\n",
      "epochs =  28 , step =  400 , loss_val =  0.24907474\n",
      "epochs =  28 , step =  500 , loss_val =  0.025392942\n",
      "epochs =  29 , step =  0 , loss_val =  1.0826005e-05\n",
      "epochs =  29 , step =  100 , loss_val =  0.37298447\n",
      "epochs =  29 , step =  200 , loss_val =  0.093984835\n",
      "epochs =  29 , step =  300 , loss_val =  0.08727748\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize tf.Variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        total_batch = int(mnist.train.num_examples / batch_size)   # 55,000 / 100\n",
    "        \n",
    "        for step in range(total_batch):\n",
    "            \n",
    "            batch_x_data, batch_t_data = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            loss_val, _ = sess.run([loss, train], feed_dict={X: batch_x_data, T: batch_t_data})\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print(\"epochs = \", i, \", step = \", step, \", loss_val = \", loss_val)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    print(\"\\nelapsed time = \", end_time - start_time)\n",
    "    \n",
    "    # Accuracy\n",
    "    test_x_data = mnist.test.images     # 10000 * 784\n",
    "    test_t_data = mnist.test.labels     # 10000 * 10\n",
    "    \n",
    "    accuracy_val = sess.run(accuracy, feed_dict={X: test_x_data, T: test_t_data})\n",
    "    \n",
    "    print(\"\\nAccuracy = \", accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
